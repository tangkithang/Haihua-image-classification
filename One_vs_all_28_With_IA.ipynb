{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3dc019fa3da4fecb4b98564239a7fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68ae6bcc1c2f438887fc286ce52a6125",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_74e730f477b74af8a61158566030ced8",
              "IPY_MODEL_fcc440f0ad544f568118fb0d8ebf0aa3"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "68ae6bcc1c2f438887fc286ce52a6125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "74e730f477b74af8a61158566030ced8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1a7681b53d1348feb50bc4e21a5d2915",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 100441675,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100441675,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7ba2873de4f4798af8667a1b3c15388"
          },
          "model_module_version": "1.5.0"
        },
        "fcc440f0ad544f568118fb0d8ebf0aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d5d20213c034486a9e7e2c07540ea5f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 95.8M/95.8M [00:13&lt;00:00, 7.45MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c429c099796148b09c6eb56a2cbafe6a"
          },
          "model_module_version": "1.5.0"
        },
        "1a7681b53d1348feb50bc4e21a5d2915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "e7ba2873de4f4798af8667a1b3c15388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "d5d20213c034486a9e7e2c07540ea5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "c429c099796148b09c6eb56a2cbafe6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SToisLsAPwi8"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import imageio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mean  \n",
        "from math import ceil  \n",
        "from matplotlib import pyplot as plt\n",
        "import cv2 as cv\n",
        "from google.colab import files\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "import random\n",
        "from skimage.util import random_noise\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "b1e1e134-cba4-4cb2-8d92-999ea8ce1e20",
        "id": "ZIQezKRiPwi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "a6f5a684-2420-4d35-b3a3-fc815ec3a7af",
        "id": "-HggNA-RPwiv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!unzip '/content/drive/Shared drives/HaiHua Garbage Classification /Justin/Train_Resized_224.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",

            "  inflating: Train_Resized_224/93982.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX8BkbFREO2d"
      },
      "source": [
        "### This dataloader returns a y_in containing [[0,1]] OR [[1,0]] (dim=1 compatible) instead of the usual 204 classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybfe3uWtFTyX"
      },
      "source": [
        "**Modify the targeting_class variable according to your needs! **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKu3neTxhvdX"
      },
      "source": [
        "def dataloader(batchsize): \n",
        "  labels = pd.read_csv(\n",
        "      '/content/drive/Shared drives/HaiHua Garbage Classification /Train_data_320.csv')  \n",
        "\n",
        "  targeting_class = 28 \n",
        "  #                 ^ modify the targeting_class according to your needs! \n",
        "\n",
        "\n",
        "  idx = labels['image_id']  \n",
        "  labels = labels.set_index('image_id') \n",
        "  np.random.shuffle(idx)  \n",
        "\n",
        "  x = [] \n",
        "  y = []  \n",
        "  #random = 10\n",
        "  for i, image_name in enumerate(idx):  \n",
        "      if i % batchsize == 0 and i != 0:  \n",
        "          x_in = torch.tensor(x, dtype=torch.float32).cuda()  \n",
        "          y_in = torch.tensor(y, dtype=torch.float32).cuda()  \n",
        "          x = [] \n",
        "          y = [] \n",
        "          yield (x_in, y_in) \n",
        "\n",
        "      image_path = '/content/Train_Resized_224/%s.png' % image_name  \n",
        "      image = cv.imread(image_path) \n",
        "      number = random.randint(0,10)\n",
        "      if number <= 5:\n",
        "        noisy_image = image + np.random.rand(224, 224, 3)*30\n",
        "        noisy_image = np.divide(noisy_image, 255)\n",
        "        noisy_image = np.subtract(noisy_image, np.array([0.485, 0.456, 0.406]))\n",
        "        noisy_image = np.divide(noisy_image, np.array([0.229, 0.224, 0.225]))\n",
        "        image = np.reshape(noisy_image, (3,224,224))\n",
        "\n",
        "      else: \n",
        "        image = np.divide(image, 255)\n",
        "        image = np.subtract(image, np.array([0.485, 0.456, 0.406]))\n",
        "        image = np.divide(image, np.array([0.229, 0.224, 0.225]))\n",
        "        image = np.reshape(image, (3, 224, 224))\n",
        "\n",
        "      \n",
        "      id = labels.at[int(image_name), 'id']  \n",
        "      x.append(image)  \n",
        "      \n",
        "      y0 = np.zeros(2)  \n",
        "      if int(id) != int(targeting_class): \n",
        "        y0[0]=1\n",
        "      else:\n",
        "        y0[1]=1\n",
        "\n",
        "      y.append(y0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd6G_HWHPwie"
      },
      "source": [
        "def get_accuracy(prediction, target):  \n",
        "\n",
        "    pred = torch.argmax(prediction, dim=1) \n",
        "    true = torch.argmax(target, dim=1) \n",
        "    n_correct = torch.sum(torch.eq(pred, true)) \n",
        "    n_total = len(pred) \n",
        "    accuracy = n_correct / n_total\n",
        "    \n",
        "    return accuracy.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leNLf5wpE0w0"
      },
      "source": [
        "### DO NOT LOAD 204-class .pt models, cuz they are incompatible!! Only load previously trained binary-class models! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "4e86e5df-f1c9-4590-8523-fa3798ad24b1",
        "id": "5gZij1oPPwiF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e3dc019fa3da4fecb4b98564239a7fad",
            "68ae6bcc1c2f438887fc286ce52a6125",
            "74e730f477b74af8a61158566030ced8",
            "fcc440f0ad544f568118fb0d8ebf0aa3",
            "1a7681b53d1348feb50bc4e21a5d2915",
            "e7ba2873de4f4798af8667a1b3c15388",
            "d5d20213c034486a9e7e2c07540ea5f7",
            "c429c099796148b09c6eb56a2cbafe6a"
          ]
        }
      },
      "source": [
        "BATCHSIZE = 256 \n",
        "EPOCH = 32\n",
        "best_acc = 0\n",
        "torch.backends.cudnn.enabled = False\n",
        "n_train = ceil(70000 / BATCHSIZE) \n",
        "n_batches = ceil(80000 / BATCHSIZE)\n",
        "\n",
        "print('loading model...')\n",
        "model = torchvision.models.resnext50_32x4d(pretrained=True)\n",
        "\n",
        "\n",
        "for i, child in enumerate(model.children()): \n",
        "    if i < 7:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "model.fc = torch.nn.Linear(2048, 256, bias=True)\n",
        "\n",
        "model = torch.nn.Sequential(model, torch.nn.Linear(256,2, bias=True), torch.nn.Softmax(dim=1)).to('cuda')\n",
        "\n",
        "\n",
        "model = torch.load(f'/content/drive/Shared drives/HaiHua Garbage Classification /Justin pretrained networks/One-VS-All/Apr12_ONE_VS_ALL_LABEL28_WITH_IA.pt')\n",
        "# ^ DO NOT LOAD MODELS UNLESS THEY ARE TRAINED BINARY, ELSE ERRORS WOULD OCCUR\n",
        "\n",
        "\n",
        "print('training...')\n",
        "print('trains for  %s steps per epoch' % n_train)\n",
        "optimizer = torch.optim.Adam(lr=0.0001, params=list(model.parameters()))\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "\n",
        "    tbar = tqdm(total=n_batches)\n",
        "    for i, (x, y) in enumerate(dataloader(BATCHSIZE)):\n",
        "        if i < n_train:\n",
        "            model.train()\n",
        "            model.zero_grad()\n",
        "            output = model(x) \n",
        "            loss = criterion(output, y)\n",
        "            acc = get_accuracy(output, y)\n",
        "            loss.backward(retain_graph=False)\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            train_acc.append(acc)\n",
        "            tbar.set_description('loss %s' % round(mean(train_loss), 10))\n",
        "            tbar.update()            \n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "\n",
        "                model.zero_grad()\n",
        "                output = model(x)\n",
        "                loss = criterion(output, y)\n",
        "                acc = get_accuracy(output, y)\n",
        "\n",
        "                val_loss.append(loss.item())\n",
        "                val_acc.append(acc)\n",
        "\n",
        "            tbar.set_description('loss %s' % round(mean(val_loss), 10))\n",
        "            tbar.update()                \n",
        "\n",
        "        if i % 50 == 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                print('Current lr: %s' % param_group['lr'])\n",
        "            print(i, loss.item(), acc)\n",
        "\n",
        "    current_acc = ((mean(train_acc) + mean(val_acc)) / 2)\n",
        "    if current_acc > best_acc:\n",
        "      current_acc = best_acc \n",
        "\n",
        "\n",
        "\n",
        "      model_save_name = 'Apr12_ONE_VS_ALL_LABEL28_WITH_IA.pt'\n",
        "      #                    ^^ please remember to modify the file name to be saved! Do not duplicate file names to avoid error \n",
        "      # remember to save it as a pt file !!! \n",
        "\n",
        "\n",
        "\n",
        "      path = F'/content/drive/Shared drives/HaiHua Garbage Classification /Justin pretrained networks/One-VS-All/{model_save_name}'\n",
        "      torch.save(model, path)\n",
        "      print('whole model saved')\n",
        "\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    print('Epoch: %s\\n'\n",
        "          'LOSS:\\tTrain: %s\\tVal: %s\\n'\n",
        "          'ACC:\\tTrain: %s\\tVal: %s' % (epoch+1, mean(train_loss), mean(val_loss), mean(train_acc), mean(val_acc)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/checkpoints/resnext50_32x4d-7cdf4587.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3dc019fa3da4fecb4b98564239a7fad",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=100441675), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "trains for  274 steps per epoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.0102626067:   0%|          | 1/313 [00:15<1:19:13, 15.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "0 0.010262606665492058 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.0052628757:  16%|█▋        | 51/313 [10:04<51:14, 11.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "50 0.0016922433860599995 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.0042417882:  32%|███▏      | 101/313 [19:54<41:40, 11.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "100 0.001837648218497634 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.0036265654:  48%|████▊     | 151/313 [29:38<31:28, 11.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "150 0.0061227441765367985 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.0031611483:  64%|██████▍   | 201/313 [39:18<21:35, 11.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "200 0.0005791323492303491 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.0030058599:  80%|████████  | 251/313 [48:56<11:56, 11.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "250 5.020800017518923e-05 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.0012860139:  96%|█████████▌| 301/313 [57:47<01:57,  9.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "300 0.0008451691828668118 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 0.002187678: 100%|█████████▉| 312/313 [59:35<00:09,  9.81s/it]\n",
            "  0%|          | 0/313 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "whole model saved\n",
            "Epoch: 1\n",
            "LOSS:\tTrain: 0.0033404777967573195\tVal: 0.0021876779559725514\n",
            "ACC:\tTrain: 0.7481751824817519\tVal: 0.8421052631578947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loss 0.0005342436:   0%|          | 0/313 [00:14<?, ?it/s]\u001b[A\n",
            "loss 0.0005342436:   0%|          | 1/313 [00:14<1:13:27, 14.13s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current lr: 0.0001\n",
            "0 0.0005342435906641185 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
